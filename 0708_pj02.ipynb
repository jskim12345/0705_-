{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1차 데이터 전처리 특성중요도 0.01미만 54개 제거\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = pd.read_csv('./train_data.csv')\n",
    "\n",
    "# 특성과 타겟 변수 분리\n",
    "data_input = data.drop(columns=['Bankrupt?'])\n",
    "data_target = data['Bankrupt?']\n",
    "\n",
    "# 데이터 분리\n",
    "train_input, test_input, train_target, test_target = train_test_split(\n",
    "    data_input, data_target, random_state=42)\n",
    "\n",
    "# 랜덤 포레스트 모델 학습\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(train_input, train_target)\n",
    "\n",
    "# 특성 중요도 확인\n",
    "importances = rf_model.feature_importances_\n",
    "features = data_input.columns\n",
    "feature_importances = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# 중요도가 낮은 특성 제거 (중요도 0.01 미만인 특성 제거)\n",
    "important_features = feature_importances[feature_importances['Importance'] > 0.01]['Feature']\n",
    "data_input_reduced = data_input[important_features]\n",
    "\n",
    "# 중요한 특성만 포함한 데이터프레임에 타겟 변수 'Bankrupt?' 컬럼 추가\n",
    "data_reduced_with_target = data_input_reduced.copy()\n",
    "data_reduced_with_target['Bankrupt?'] = data_target\n",
    "\n",
    "\n",
    "data_reduced_with_target.to_csv('./train_data_reduced.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./test_data_reduced.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 제거할 특성 불러오기\n",
    "low_importance_features = pd.read_csv('./low_importance_features.csv')['Feature'].tolist()\n",
    "\n",
    "# 새로운 데이터 불러오기\n",
    "test_data = pd.read_csv('./test_data.csv')\n",
    "\n",
    "# 제거할 특성을 데이터에서 제거\n",
    "test_data_reduced = test_data.drop(columns=low_importance_features)\n",
    "\n",
    "# 중요한 특성만 포함된 새로운 데이터 저장\n",
    "file_path_test_reduced = './test_data_reduced.csv'\n",
    "test_data_reduced.to_csv(file_path_test_reduced, index=False)\n",
    "\n",
    "file_path_test_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./test_data_reduced.csv')\n",
    "ntest_input = test_data.drop(columns=['Bankrupt?'])\n",
    "ntest_target = test_data['Bankrupt?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 하이퍼파라미터: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 6, 'min_samples_split': 10, 'n_estimators': 100}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous-multioutput and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 모델 평가 - 정확도 (하이퍼파라미터 튜닝 후)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m train_accuracy_tuned \u001b[38;5;241m=\u001b[39m accuracy_score(train_target_reduced, train_predictions_tuned)\n\u001b[1;32m---> 48\u001b[0m test_accuracy_tuned \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mntest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntest_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m특성 제거 후 모델 성능:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m훈련 세트 정확도 (하이퍼파라미터 튜닝 후): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy_tuned\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:222\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:108\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    105\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    110\u001b[0m             type_true, type_pred\n\u001b[0;32m    111\u001b[0m         )\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    115\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous-multioutput and binary targets"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('./train_data_reduced.csv')\n",
    "\n",
    "# 데이터 분리 (중요도가 낮은 특성 제거 후)\n",
    "train_input_reduced, test_input_reduced, train_target_reduced, test_target_reduced = train_test_split(\n",
    "    data_input_reduced, data_target, random_state=42)\n",
    "\n",
    "# 랜덤 포레스트 모델 재학습\n",
    "rf_model_reduced = RandomForestClassifier(random_state=42)\n",
    "rf_model_reduced.fit(train_input_reduced, train_target_reduced)\n",
    "\n",
    "\n",
    "# 3. 하이퍼파라미터 튜닝\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [10, 15],\n",
    "    'min_samples_leaf': [4, 6],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1,\n",
    "                           scoring='accuracy')\n",
    "grid_search.fit(train_input_reduced, train_target_reduced)\n",
    "\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"최적의 하이퍼파라미터:\", grid_search.best_params_)\n",
    "\n",
    "# 최적의 하이퍼파라미터로 모델 재학습\n",
    "best_rf_model.fit(train_input_reduced, train_target_reduced)\n",
    "\n",
    "# 예측 (하이퍼파라미터 튜닝 후)\n",
    "train_predictions_tuned = best_rf_model.predict(train_input_reduced)\n",
    "test_predictions_tuned = best_rf_model.predict(test_input_reduced)\n",
    "\n",
    "# 모델 평가 - 정확도 (하이퍼파라미터 튜닝 후)\n",
    "train_accuracy_tuned = accuracy_score(train_target_reduced, train_predictions_tuned)\n",
    "test_accuracy_tuned = accuracy_score(ntest_input, ntest_target)\n",
    "\n",
    "\n",
    "print(\"특성 제거 후 모델 성능:\")\n",
    "print(f\"훈련 세트 정확도 (하이퍼파라미터 튜닝 후): {train_accuracy_tuned}\")\n",
    "print(f\"테스트 세트 정확도 (하이퍼파라미터 튜닝 후): {test_accuracy_tuned}\")\n",
    "\n",
    "# 교차 검증 (Cross-Validation, 하이퍼파라미터 튜닝 후)\n",
    "cv_scores_tuned = cross_val_score(best_rf_model, data_input_reduced, data_target, cv=5)\n",
    "cv_accuracy_tuned = np.mean(cv_scores_tuned)\n",
    "\n",
    "# 교차 검증 평가 - MSE, MAE (하이퍼파라미터 튜닝 후)\n",
    "cv_predictions_tuned = cross_val_predict(best_rf_model, data_input_reduced, data_target, cv=5)\n",
    "\n",
    "print(\"교차 검증 평균 정확도 (하이퍼파라미터 튜닝 후): \", cv_accuracy_tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
